# -*- coding: utf-8 -*-
"""СИИ8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KVKQZG0nfce7WxvxEydQ59hsa2iqUcg0
"""



import tensorflow as tf

# Configure GPU memory growth
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Set memory growth to avoid allocating all memory at once
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        
        # Or set memory limit
        # tf.config.experimental.set_virtual_device_configuration(
        #     gpus[0],
        #     [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]
        # )
    except RuntimeError as e:
        print(e)



import tensorflow as tf
import numpy as np

# Force GPU usage
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print("Using GPU:", physical_devices[0])
else:
    print("No GPU found, using CPU")

# Create a simple model and force GPU training
with tf.device('/GPU:0'):


    import numpy
    from keras.models import Sequential
    from keras.layers import Dense
    from keras.layers import Dropout
    from keras.layers import LSTM
    from keras.callbacks import ModelCheckpoint
    from tensorflow.keras.utils import to_categorical # Опять 25

    filename = "wonderland.txt"
    raw_text = open(filename, encoding='utf-8').read()
    raw_text = raw_text.lower()

    chars = sorted(list(set(raw_text)))
    char_to_int = dict((c, i) for i, c in enumerate(chars))

    n_chars = len(raw_text)
    n_vocab = len(chars)
    print("Total Characters: ", n_chars)
    print("Total Vocab: ", n_vocab)

    """Мы видим, что книга содержит менее 150000 символов и что при преобразовании в
    строчные буквы в словаре есть только 47 различных символов для изучения сетью.
    Гораздо больше, чем 26 в алфавите.

    Это не совпало с моими данными. Любопытно...
    """

    seq_length = 100
    dataX = []
    dataY = []
    for i in range(0, n_chars - seq_length, 1):
            seq_in = raw_text[i:i + seq_length]
            seq_out = raw_text[i + seq_length]
            dataX.append([char_to_int[char] for char in seq_in])
            dataY.append(char_to_int[seq_out])
    n_patterns = len(dataX)
    print("Total Patterns: ", n_patterns)

    X = numpy.reshape(dataX, (n_patterns, seq_length, 1))
    X = X / float(n_vocab)
    y = to_categorical(dataY)

    model = Sequential()
    model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))
    model.add(Dropout(0.2))
    model.add(Dense(y.shape[1], activation='softmax'))
    model.compile(loss='categorical_crossentropy',
    optimizer='adam')

    # define the checkpoint
    filepath="weights-improvement-{epoch:02d}-{loss:.4f}.keras"  # .keras instead of .hdf5
    checkpoint = ModelCheckpoint(filepath, monitor='loss',
    verbose=1, save_best_only=True, mode='min')
    callbacks_list = [checkpoint]

    # ХАХААХАХААХААХ 3 часа тренировки! УРА!

    model.fit(X, y, epochs=20, batch_size=128,
    callbacks=callbacks_list)